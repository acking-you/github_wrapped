import json
import math
import re
import datetime as dt
from collections import Counter, defaultdict
from dataclasses import dataclass
from pathlib import Path
from zoneinfo import ZoneInfo


BASE = Path("data/github-wrapped-2024")
RAW = BASE / "raw"
OUT_DIR = BASE / "processed"
OUT_DIR.mkdir(parents=True, exist_ok=True)

YEAR = 2024
TZ = ZoneInfo("Asia/Shanghai")


def load(path: Path):
    return json.loads(path.read_text(encoding="utf-8"))


def parse_iso8601(ts: str) -> dt.datetime:
    if not ts:
        raise ValueError("empty timestamp")
    if ts.endswith("Z"):
        ts = ts[:-1] + "+00:00"
    return dt.datetime.fromisoformat(ts)


def to_local(ts: str) -> dt.datetime:
    return parse_iso8601(ts).astimezone(TZ)


def safe_int(value, default=0) -> int:
    try:
        return int(value)
    except Exception:
        return default


def clamp(value: float, low: float, high: float) -> float:
    return max(low, min(high, value))


def normalize(value: float, max_value: float) -> float:
    if max_value <= 0:
        return 0.0
    return float(value) / float(max_value)


def compact_number(n: int) -> str:
    if n >= 100_000_000:
        return f"{n/100_000_000:.1f}‰∫ø"
    if n >= 10_000:
        return f"{n/10_000:.1f}‰∏á"
    return str(n)


@dataclass(frozen=True)
class Category:
    key: str
    label: str
    emoji: str
    color: str
    description: str


"""
Star ÊäÄÊúØÈõ∑ËææÔºöÈ¢ÜÂüüÂàÜÁ±ª

ËØ¥ÊòéÔºö
- ËøôÈáåÁöÑÂàÜÁ±ªÂÆåÂÖ®Âü∫‰∫é‰ªìÂ∫ìÁöÑ `description` + `topics`ÔºàÂÖ∂Ê¨°ÊâçÂèÇËÄÉËØ≠Ë®Ä/ÂêçÂ≠óÔºâ„ÄÇ
- ÁõÆÊ†áÊòØÊää 2024 Âπ¥ Star ÁöÑ 300+ È°πÁõÆÂÅöÂá∫‚ÄúÈ¢ÜÂüü‚ÄùÂàíÂàÜÔºåËÄå‰∏çÊòØËØ≠Ë®Ä/ÁîüÊÄÅÁªÜÂàÜ„ÄÇ
"""


# 26 ‰∏™È¢ÜÂüüÂàÜÁ±ªÔºàÂèØÊâ©Â±ïÔºõ‰øùÊåÅ key Á®≥ÂÆö‰æø‰∫éÂâçÁ´ØÊ∏≤ÊüìÔºâ
CATEGORIES = [
    # AI / LLM
    Category(key="ai_inference", label="Ê®°ÂûãÊé®ÁêÜ/ËøêË°åÊó∂", emoji="‚öôÔ∏è", color="#f97316", description="ollama, llama.cpp, mistral.rs, LocalAI"),
    Category(key="ai_rag_agent", label="RAG/Agent Â∑•ÂÖ∑Èìæ", emoji="üß†", color="#8b5cf6", description="LangChain, GraphRAG, MCP, tool-calling"),
    Category(key="ai_platform", label="AI Âπ≥Âè∞/Â∑•‰ΩúÊµÅ", emoji="üß©", color="#0ea5e9", description="Dify, Langflow, Flowise, FastGPT"),
    Category(key="ai_apps", label="AI Â∫îÁî®/ÂÆ¢Êà∑Á´Ø", emoji="üí¨", color="#f43f5e", description="Open WebUI, NextChat, Tabby, AnythingLLM"),

    # Êï∞ÊçÆ / Êï∞ÊçÆÂ∫ì
    Category(key="db_systems", label="Êï∞ÊçÆÂ∫ìÁ≥ªÁªü", emoji="üóÑÔ∏è", color="#22c55e", description="Postgres/Neon, Êó∂Â∫è/Âõæ/ÂàÜÂ∏ÉÂºèÊï∞ÊçÆÂ∫ì"),
    Category(key="db_storage", label="Â≠òÂÇ®ÂºïÊìé/ÂµåÂÖ•Âºè DB", emoji="üß±", color="#eab308", description="RocksDB/LSM, embedded database, storage engine"),
    Category(key="db_tooling", label="SQL/ORM/Êï∞ÊçÆÂ∫ìÂ∑•ÂÖ∑", emoji="üß∞", color="#fb923c", description="sqlx, sea-orm, sqlglot, Chat2DB"),
    Category(key="vector_search", label="ÂêëÈáèÊ£ÄÁ¥¢/Embedding", emoji="üß≤", color="#a855f7", description="Qdrant, Milvus, pgvector, ANN/embedding"),
    Category(key="data_analytics", label="ÂàÜÊûêÂºïÊìé/Êï∞ÊçÆËÆ°ÁÆó", emoji="üìà", color="#06b6d4", description="OLAP, DataFusion, Polars, data warehouse/lakehouse"),
    Category(key="data_formats", label="Êï∞ÊçÆÊ†ºÂºè/Ëß£Êûê/Â∫èÂàóÂåñ", emoji="üßæ", color="#14b8a6", description="Arrow/Parquet, serde/protobuf, parser/format"),
    Category(key="messaging_stream", label="Ê∂àÊÅØÈòüÂàó/ÊµÅÂ§ÑÁêÜ", emoji="üì¨", color="#38bdf8", description="MQ, streaming, pubsub, event processing"),
    Category(key="search_index", label="ÊêúÁ¥¢/Á¥¢Âºï/Ê£ÄÁ¥¢", emoji="üîé", color="#f472b6", description="search engine, index, full-text, filters"),

    # Âü∫Á°ÄËÆæÊñΩ
    Category(key="storage_fs", label="ÂØπË±°Â≠òÂÇ®/Êñá‰ª∂Á≥ªÁªü", emoji="ü™£", color="#60a5fa", description="S3/MinIO, Ceph, JuiceFS, SeaweedFS"),
    Category(key="cloud_devops", label="ÂÆπÂô®/K8s/DevOps", emoji="‚òÅÔ∏è", color="#93c5fd", description="Docker/Podman, Kubernetes, CI/CD, cloud-native"),
    Category(key="proxy_vpn", label="‰ª£ÁêÜ/VPN/ÁΩëÁªúÂÆ¢Êà∑Á´Ø", emoji="üõ°Ô∏è", color="#34d399", description="Clash, proxy/VPN clients, tunneling"),
    Category(key="network_protocols", label="ÁΩëÁªúÂçèËÆÆ/‰º†ËæìÂ∫ì", emoji="üåê", color="#22d3ee", description="QUIC/HTTP3, gRPC, WebSocket, TCP/IP, RDMA"),

    # Á≥ªÁªü / ÊÄßËÉΩ / ÂÆâÂÖ®
    Category(key="observability_perf", label="ÂèØËßÇÊµãÊÄß/ÊÄßËÉΩÂàÜÊûê", emoji="üìä", color="#fbbf24", description="benchmark, flamegraph/pprof, tracing/metrics"),
    Category(key="security", label="ÂÆâÂÖ®/ÁΩëÁªúÊâ´Êèè", emoji="üîê", color="#ef4444", description="port scanner, pentest, traffic inspection"),
    Category(key="systems_os", label="Êìç‰ΩúÁ≥ªÁªü/ËôöÊãüÂåñ/ÂÜÖÊ†∏", emoji="üß¨", color="#a78bfa", description="kernel, WSL/VM, LibOS, OS/runtime"),
    Category(key="systems_libs", label="È´òÊÄßËÉΩÁ≥ªÁªüÂ∫ì", emoji="‚öôÔ∏è", color="#3b82f6", description="folly/abseil, hashmaps, coroutine/async primitives"),

    # ÂºÄÂèëÂ∑•ÂÖ∑
    Category(key="lang_tooling", label="ËØ≠Ë®ÄÂ∑•ÂÖ∑Èìæ/ÁºñËØëÂô®", emoji="üß™", color="#c084fc", description="compiler/interpreter, linter/formatter, LLVM/Cranelift"),
    Category(key="build_release", label="ÊûÑÂª∫/ÂåÖÁÆ°ÁêÜ/ÂèëÂ∏É", emoji="üì¶", color="#fdba74", description="cargo, rye/pixi/uv, cross-compile, build tooling"),
    Category(key="editor_ide", label="ÁºñËæëÂô®/IDE", emoji="‚úèÔ∏è", color="#a3e635", description="Zed, Helix, Neovim, VS Code tooling"),
    Category(key="cli_terminal", label="CLI/ÁªàÁ´ØÂ∑•ÂÖ∑", emoji="‚å®Ô∏è", color="#67e8f9", description="nushell, atuin, gitui, terminal utilities"),
    Category(key="ui_frameworks", label="UI/Ë∑®Á´ØÊ°ÜÊû∂", emoji="üñºÔ∏è", color="#fb7185", description="Tauri/Flutter/Avalonia, React/Svelte/Solid, GUI frameworks"),

    # Â∫îÁî®‰∏éËµÑÊñô
    Category(key="apps_productivity", label="Â∫îÁî®/ÊïàÁéáÂ∑•ÂÖ∑", emoji="üß©", color="#f9a8d4", description="PDF/ÁøªËØë/OCR/Êñá‰ª∂ÁÆ°ÁêÜ/Ê°åÈù¢Â∑•ÂÖ∑"),
    Category(key="media", label="Â™í‰Ωì/ÂΩ±Èü≥/Â®±‰πê", emoji="üé¨", color="#fda4af", description="IPTV, music/video apps, media utilities"),
    Category(key="learning", label="Â≠¶‰π†/ËµÑÊñô/Ê∏ÖÂçï", emoji="üìö", color="#fde047", description="books, tutorials, papers, curated lists"),
]


CATEGORY_BY_KEY = {c.key: c for c in CATEGORIES}


def _tokenize(text: str) -> set[str]:
    # Keep hyphenated tokens used by GitHub topics (e.g., "vector-database").
    raw = set(re.findall(r"[a-z0-9][a-z0-9_+.#\\-]*", text.lower()))
    parts: set[str] = set()
    for t in raw:
        parts.update(x for x in re.split(r"[-_.+/]+", t) if x)
    return raw | parts


def _phrase_match(*, phrase: str, text: str, tokens: set[str]) -> bool:
    p = (phrase or "").strip().lower()
    if not p:
        return False

    # Chinese (and other non-ASCII) phrases: substring match is usually intended.
    if any(ord(ch) > 127 for ch in p):
        return p in text

    # Multi-word ASCII phrases: keep substring semantics.
    if re.search(r"\\s", p):
        return p in text

    # Short ASCII tokens like IDE/ORM/SQL: require token match to avoid false positives
    # (e.g. "orm" in "performance", "ide" in "side").
    if len(p) <= 3:
        return p in tokens

    return p in tokens or p in text


# ÂàÜÁ±ªËßÑÂàôÔºàtopics ‰∏∫Âº∫‰ø°Âè∑Ôºõtokens/phrases ‰∏∫Âº±‰ø°Âè∑ÔºõÂ∞ΩÈáèÁî® description Êù•ÂÜ≥ÂÆö‚ÄúÈ¢ÜÂüü‚ÄùÔºâ
RULES: dict[str, dict] = {
    "ai_inference": {
        "topics": {"ollama", "llama", "ggml", "inference", "cuda", "tensorrt", "triton", "onnx", "gguf", "diffusion", "stable-diffusion"},
        "tokens": {"ollama", "llama", "mistral", "inference", "ggml", "gguf", "diffusion", "stable-diffusion", "quantization", "quantize", "tensorrt", "triton", "onnx", "cuda", "openvino", "localai", "deepseek", "qwen", "gemma"},
        "phrases": {"Ê®°ÂûãÊé®ÁêÜ", "Êé®ÁêÜÂºïÊìé", "Êé®ÁêÜÂä†ÈÄü", "Ê®°ÂûãÈÉ®ÁΩ≤", "Á¶ªÁ∫øËøêË°å", "Êú¨Âú∞Ê®°Âûã", "ÈáèÂåñ"},
        "regex": [re.compile(r"\b(inference|quantiz|ggml|gguf|llama\.cpp|ollama|mistral)\b", re.I)],
    },
    "ai_rag_agent": {
        "topics": {"rag", "agent", "agents", "mcp", "prompt", "langchain", "llamaindex", "graphrag"},
        "tokens": {"rag", "agent", "agents", "prompt", "tool-calling", "function-calling", "langchain", "llamaindex", "graphrag", "mcp", "openai", "claude", "anthropic"},
        "phrases": {"Áü•ËØÜÂ∫ì", "Ê£ÄÁ¥¢Â¢ûÂº∫", "Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê", "ÂêëÈáèÊ£ÄÁ¥¢", "ÊèêÁ§∫ËØç", "Â∑•ÂÖ∑Ë∞ÉÁî®", "Êô∫ËÉΩ‰Ωì", "‰ª£ÁêÜ"},
        "regex": [re.compile(r"\b(rag|retrieval[- ]augmented|agent|tool[- ]calling|prompt)\b", re.I)],
    },
    "ai_platform": {
        "topics": {"dify", "langflow", "flowise", "low-code", "no-code", "workflow-automation", "agentic-workflow", "agentic-ai"},
        "tokens": {"workflow", "orchestration", "studio", "visual", "builder", "low-code", "nocode", "deploy", "production", "agentic", "dify", "langflow", "flowise", "fastgpt"},
        "phrases": {"Â∑•‰ΩúÊµÅ", "ÁºñÊéí", "‰Ωé‰ª£Á†Å", "Â∫îÁî®ÂºÄÂèë"},
        "regex": [re.compile(r"\b(workflow|orchestrat|low[- ]code|no[- ]code|visual)\b", re.I)],
    },
    "ai_apps": {
        "topics": set(),
        "tokens": {"assistant", "chat", "client", "webui", "ui", "desktop", "copilot", "tabby", "anything-llm", "nextchat", "open-webui", "jan"},
        "phrases": {"AI Âä©Êâã", "ËÅäÂ§©", "ÂÆ¢Êà∑Á´Ø", "Ê°åÈù¢Â∫îÁî®", "Êú¨Âú∞ËøêË°å", "Ëá™ÊâòÁÆ°", "Áü•ËØÜÂ∫ìÂ∫îÁî®"},
        "regex": [re.compile(r"\b(ai assistant|chat(\s|-)?(ui|client)|self-hosted)\b", re.I)],
    },
    "vector_search": {
        "topics": {"vector-database", "vector-search", "embedding", "ann", "qdrant", "milvus", "faiss", "chroma", "pgvector"},
        "tokens": {"vector", "embedding", "ann", "faiss", "qdrant", "milvus", "chroma", "pgvector", "similarity", "semantic"},
        "phrases": {"ÂêëÈáèÊï∞ÊçÆÂ∫ì", "ÂêëÈáèÊ£ÄÁ¥¢", "ËØ≠‰πâÊ£ÄÁ¥¢", "Áõ∏‰ººÂ∫¶Ê£ÄÁ¥¢", "ÂµåÂÖ•"},
        "regex": [re.compile(r"\b(vector (database|search)|embedding|ann|nearest neighbor)\b", re.I)],
    },
    "data_analytics": {
        "topics": {"olap", "analytics", "big-data", "datawarehouse", "lakehouse", "clickhouse", "duckdb", "datafusion", "polars", "delta-lake", "mpp", "sql"},
        "tokens": {"olap", "analytics", "warehouse", "lakehouse", "query-engine", "query", "datafusion", "polars", "duckdb", "clickhouse", "trino", "presto", "spark", "delta", "mpp"},
        "phrases": {"Êï∞ÊçÆ‰ªìÂ∫ì", "Êπñ‰ªì", "ÂàÜÊûêÂºïÊìé", "Êü•ËØ¢ÂºïÊìé", "ÂàóÂºè", "OLAP"},
        "regex": [re.compile(r"\b(olap|data warehouse|lakehouse|query engine|columnar)\b", re.I)],
    },
    "data_formats": {
        "topics": {"arrow", "parquet", "protobuf", "serialization", "parser"},
        "tokens": {"arrow", "parquet", "protobuf", "serde", "serialization", "serialize", "serializer", "deserialize", "deserializer", "json", "yaml", "toml", "csv", "bincode", "flatbuffers", "capnp", "capnproto", "parser", "lexer", "grammar", "format"},
        "phrases": {"Â∫èÂàóÂåñ", "ÂèçÂ∫èÂàóÂåñ", "Ëß£ÊûêÂô®", "ËØ≠Ê≥ï", "Ê†ºÂºè", "ÁºñÁ†Å", "ÂçèËÆÆ"},
        "regex": [re.compile(r"\b(arrow|parquet|protobuf|serializ|deserializ|flatbuffers)\b", re.I)],
    },
    "db_systems": {
        "topics": {"database", "dbms", "distributed-systems", "graph-database", "timeseries", "time-series"},
        "tokens": {"database", "dbms", "postgres", "postgresql", "mysql", "redis", "serverless", "cluster", "distributed", "replication", "sharding", "timeseries", "time-series"},
        "phrases": {"Êï∞ÊçÆÂ∫ì", "ÂàÜÂ∏ÉÂºèÊï∞ÊçÆÂ∫ì", "Êó∂Â∫èÊï∞ÊçÆÂ∫ì", "ÂõæÊï∞ÊçÆÂ∫ì", "Â≠òÂÇ®Á≥ªÁªü"},
        "regex": [re.compile(r"\b(database|dbms|time series|graph database|distributed database)\b", re.I)],
    },
    "db_storage": {
        "topics": {"storage-engine", "embedded-database", "rocksdb", "lsm-tree", "key-value", "kv", "cache"},
        "tokens": {"rocksdb", "leveldb", "lmdb", "lsm", "lsm-tree", "btree", "b-tree", "sstable", "wal", "in-process", "storage-engine", "key-value", "kv", "wisckey", "titan", "pebble"},
        "phrases": {"Â≠òÂÇ®ÂºïÊìé", "ÂµåÂÖ•ÂºèÊï∞ÊçÆÂ∫ì", "ÈîÆÂÄº", "KV", "LSM"},
        "regex": [re.compile(r"\b(rocksdb|lsm|embedded (database|storage)|storage engine|key[- ]value)\b", re.I)],
    },
    "db_tooling": {
        "topics": {"sql", "orm", "database"},
        "tokens": {"sql", "orm", "migration", "schema", "driver", "connector", "client", "admin", "toolkit", "query-builder", "sqlglot", "sqlx", "sea-orm", "chat2db"},
        "phrases": {"SQL Ëß£Êûê", "Êï∞ÊçÆÂ∫ìÂÆ¢Êà∑Á´Ø", "Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜ", "ËøÅÁßª", "ORM", "SQL Â∑•ÂÖ∑"},
        "regex": [re.compile(r"\b(sql (parser|transpil|client)|orm|migration|database client)\b", re.I)],
    },
    "messaging_stream": {
        "topics": {"streaming", "message-queue", "mq", "kafka", "nats", "rabbitmq", "pubsub"},
        "tokens": {"kafka", "nats", "rabbitmq", "mq", "message", "messaging", "queue", "stream", "streaming", "broker", "pubsub", "pulsar"},
        "phrases": {"Ê∂àÊÅØÈòüÂàó", "Ê∂àÊÅØÁ≥ªÁªü", "ÊµÅÂ§ÑÁêÜ", "Ê∂àÊÅØ‰∏≠Èó¥‰ª∂", "‰∫ã‰ª∂ÊµÅ"},
        "regex": [re.compile(r"\b(message queue|stream processing|event streaming|pubsub)\b", re.I)],
    },
    "search_index": {
        "topics": {"search", "search-engine", "full-text", "lucene", "search-engine"},
        "tokens": {"search", "index", "lucene", "inverted", "meilisearch", "zincsearch", "tantivy", "full-text", "filter", "bloom", "xor"},
        "phrases": {"ÊêúÁ¥¢", "Ê£ÄÁ¥¢", "Á¥¢Âºï", "ÂÖ®ÊñáÊ£ÄÁ¥¢"},
        "regex": [re.compile(r"\b(search engine|full[- ]text|inverted index)\b", re.I)],
    },
    "storage_fs": {
        "topics": {"s3", "object-storage", "object-store", "distributed-storage", "filesystem", "fuse", "posix", "hdfs", "ceph", "minio"},
        "tokens": {"s3", "object-storage", "objectstore", "object-store", "filesystem", "file-system", "fuse", "posix", "hdfs", "nfs", "ceph", "minio", "seaweedfs", "juicefs", "mountpoint"},
        "phrases": {"ÂØπË±°Â≠òÂÇ®", "Êñá‰ª∂Á≥ªÁªü", "ÂàÜÂ∏ÉÂºèÂ≠òÂÇ®", "‰∫ëÂ≠òÂÇ®", "ÂùóÂ≠òÂÇ®"},
        "regex": [re.compile(r"\b(s3|object (store|storage)|file system|filesystem|fuse|ceph)\b", re.I)],
    },
    "cloud_devops": {
        "topics": {"kubernetes", "k8s", "container", "cloud-native", "helm", "terraform", "podman"},
        "tokens": {"docker", "kubernetes", "k8s", "container", "containers", "helm", "terraform", "podman", "colima", "ci", "cd", "devops", "cloud-native", "runtime", "image"},
        "phrases": {"ÂÆπÂô®", "Kubernetes", "K8s", "‰∫ëÂéüÁîü", "ÈÉ®ÁΩ≤", "ÈïúÂÉè", "ÈõÜÁæ§"},
        "regex": [re.compile(r"\b(docker|kubernetes|k8s|container runtime|helm|terraform|ci/cd)\b", re.I)],
    },
    "proxy_vpn": {
        "topics": {"proxy", "vpn", "clash", "shadowsocks", "v2ray", "wireguard"},
        "tokens": {"proxy", "vpn", "clash", "shadowsocks", "v2ray", "wireguard", "socks", "tun", "tunnel"},
        "phrases": {"‰ª£ÁêÜ", "VPN", "ÁßëÂ≠¶‰∏äÁΩë", "ÁøªÂ¢ô", "‰ª£ÁêÜÂÆ¢Êà∑Á´Ø", "ÈößÈÅì"},
        "regex": [re.compile(r"\b(proxy|vpn|clash|wireguard|shadowsocks|v2ray)\b", re.I)],
    },
    "network_protocols": {
        "topics": {"networking", "quic", "grpc", "websocket", "http3", "http2", "tcp", "udp", "dns", "rdma", "kcp"},
        "tokens": {"network", "networking", "protocol", "service", "quic", "http3", "http2", "grpc", "websocket", "tcp", "udp", "dns", "rdma", "kcp", "tls", "socket", "ipstack", "http"},
        "phrases": {"ÁΩëÁªúÂçèËÆÆ", "‰º†ËæìÂçèËÆÆ", "ÁΩëÁªúÊ†à", "TCP/IP", "RDMA"},
        "regex": [re.compile(r"\b(quic|http/3|grpc|websocket|tcp/ip|rdma|kcp)\b", re.I)],
    },
    "observability_perf": {
        "topics": {"observability", "monitoring", "logging", "metrics", "tracing", "benchmark", "profiling", "pprof"},
        "tokens": {"observability", "monitoring", "metrics", "logging", "tracing", "opentelemetry", "otel", "pprof", "flamegraph", "profiler", "profiling", "benchmark", "benchmarking", "wrk", "perf", "valgrind", "debugger", "debug"},
        "phrases": {"ÂèØËßÇÊµãÊÄß", "ÁõëÊéß", "ÊåáÊ†á", "Êó•Âøó", "ÈìæË∑ØËøΩË∏™", "ÊÄßËÉΩÂàÜÊûê", "ÂéãÊµã", "Âü∫ÂáÜÊµãËØï"},
        "regex": [re.compile(r"\b(observability|monitoring|metrics|logging|tracing|pprof|flamegraph|benchmark)\b", re.I)],
    },
    "security": {
        "topics": {"security", "pentesting", "hacking", "security-tools", "packet-sniffer"},
        "tokens": {"security", "pentest", "pentesting", "hacking", "scanner", "scan", "nmap", "packet", "sniffer", "traffic", "vulnerability"},
        "phrases": {"ÂÆâÂÖ®", "Á´ØÂè£Êâ´Êèè", "ÊäìÂåÖ", "ÊºèÊ¥û", "Ê∏óÈÄè"},
        "regex": [re.compile(r"\b(port scanner|pentest|packet sniffer|vulnerability)\b", re.I)],
    },
    "systems_os": {
        "topics": {"kernel", "os", "linux-kernel", "virtualization", "vm", "wsl", "hypervisor", "libos"},
        "tokens": {"kernel", "os", "linux-kernel", "wsl", "virtualization", "virtual-machine", "vm", "hypervisor", "libos"},
        "phrases": {"ÂÜÖÊ†∏", "Êìç‰ΩúÁ≥ªÁªü", "ËôöÊãüÊú∫", "ËôöÊãüÂåñ", "Â≠êÁ≥ªÁªü", "LibOS"},
        "regex": [re.compile(r"\b(kernel|operating system|virtual machine|hypervisor|wsl|libos)\b", re.I)],
    },
    "systems_libs": {
        "topics": {"coroutines", "io-uring"},
        "tokens": {"library", "crate", "folly", "abseil", "boost", "stl", "hashmap", "btree", "b-tree", "allocator", "jemalloc", "mimalloc", "coroutine", "coroutines", "concurrency", "concurrent", "async", "runtime", "io-uring", "lock-free", "lockfree", "simd", "datastructure", "data-structure", "error", "diagnostic", "bytes", "string", "low-latency"},
        "phrases": {"È´òÊÄßËÉΩ", "Êó†ÈîÅ", "ÂçèÁ®ã", "Âπ∂Âèë", "Êï∞ÊçÆÁªìÊûÑ", "ÂÜÖÂ≠òÂàÜÈÖç", "io_uring", "ÈîôËØØÂ§ÑÁêÜ", "ËØäÊñ≠"},
        "regex": [re.compile(r"\b(hashmap|b-?tree|coroutine|concurren|io[-_ ]uring|allocator|jemalloc|mimalloc|std::)\b", re.I)],
    },
    "lang_tooling": {
        "topics": {"parser", "compiler", "interpreter"},
        "tokens": {"compiler", "interpreter", "linter", "formatter", "rustpython", "cranelift", "llvm", "miri", "rustc", "parser", "lexer", "transpiler"},
        "phrases": {"ÁºñËØëÂô®", "Ëß£ÈáäÂô®", "ËØ≠Ë®ÄÂÆûÁé∞", "ÈùôÊÄÅÂàÜÊûê", "Ê†ºÂºèÂåñ", "ËØ≠Ê≥ïËß£Êûê"},
        "regex": [re.compile(r"\b(compiler|interpreter|linter|formatter|llvm|cranelift|rustpython)\b", re.I)],
    },
    "build_release": {
        "topics": {"build", "package-manager", "cargo", "pnpm", "npm", "yarn"},
        "tokens": {"build", "package-manager", "dependency", "cargo", "cmake", "bazel", "make", "ninja", "pnpm", "npm", "yarn", "pip", "rye", "pixi", "uv", "cross", "release"},
        "phrases": {"ÊûÑÂª∫", "ÂåÖÁÆ°ÁêÜ", "‰æùËµñ", "ÂèëÂ∏É", "‰∫§ÂèâÁºñËØë"},
        "regex": [re.compile(r"\b(package manager|dependency|cross[- ]compile|build system)\b", re.I)],
    },
    "editor_ide": {
        "topics": {"editor", "ide", "vscode", "neovim", "vim", "emacs", "helix", "zed"},
        "tokens": {"editor", "ide", "vscode", "neovim", "vim", "emacs", "helix", "zed", "lsp", "language-server", "treesitter", "tree-sitter"},
        "phrases": {"ÁºñËæëÂô®", "IDE", "Êèí‰ª∂", "LSP"},
        "regex": [re.compile(r"\b(editor|ide|vscode|neovim|helix|zed)\b", re.I)],
    },
    "cli_terminal": {
        "topics": {"cli", "terminal", "shell", "tui"},
        "tokens": {"cli", "terminal", "shell", "tui", "command-line", "prompt", "bash", "zsh", "fish", "nushell", "atuin", "gitui", "ripgrep", "fd", "bat", "tokei"},
        "phrases": {"ÂëΩ‰ª§Ë°å", "ÁªàÁ´Ø", "Shell", "TUI"},
        "regex": [re.compile(r"\b(cli|command[- ]line|terminal|tui|shell)\b", re.I)],
    },
    "ui_frameworks": {
        "topics": {"tauri", "flutter", "react", "svelte", "solidjs", "wasm", "webassembly", "avalonia", "egui", "iced"},
        "tokens": {"tauri", "flutter", "electron", "avalonia", "iced", "egui", "dioxus", "leptos", "yew", "react", "svelte", "solid", "solidjs", "wasm", "webassembly", "component", "components", "ui-framework", "gui"},
        "phrases": {"UI Ê°ÜÊû∂", "GUI Ê°ÜÊû∂", "ÁªÑ‰ª∂Â∫ì", "ÂâçÁ´ØÊ°ÜÊû∂", "Ê°åÈù¢Â∫îÁî®Ê°ÜÊû∂"},
        "regex": [re.compile(r"\b(ui framework|gui framework|component library)\b", re.I)],
    },
    "apps_productivity": {
        "topics": set(),
        "tokens": {"pdf", "translator", "translation", "ocr", "resume", "filebrowser", "cleaner", "optimizer", "screenshot", "launcher", "clipboard", "desktop-environment"},
        "phrases": {"ÁøªËØë", "OCR", "PDF", "Êñá‰ª∂ÁÆ°ÁêÜ", "Ê∏ÖÁêÜ", "ÂûÉÂúæ", "ÈöêÁßÅ", "Á©∫Èó¥", "ÂÜÖÂ≠ò", "‰ºòÂåñ", "Êà™Âõæ", "Ê°åÈù¢ÁéØÂ¢É", "ÊïàÁéáÂ∑•ÂÖ∑"},
        "regex": [re.compile(r"\b(pdf|ocr|translation|resume|file browser|windows optimizer|desktop environment)\b", re.I)],
    },
    "media": {
        "topics": {"iptv", "music", "video", "streaming"},
        "tokens": {"iptv", "music", "video", "youtube", "bilibili", "streaming"},
        "phrases": {"IPTV", "Èü≥‰πê", "ËßÜÈ¢ë", "Áõ¥Êí≠", "Êí≠ÊîæÂô®"},
        "regex": [re.compile(r"\b(iptv|music|video|youtube|streaming)\b", re.I)],
    },
    "learning": {
        "topics": {"awesome", "tutorial", "book", "course", "learning", "papers"},
        "tokens": {"tutorial", "book", "course", "learning", "awesome", "papers", "reference", "handbook", "guide", "beginners", "beginner", "examples", "cheatsheet", "list", "collection"},
        "phrases": {"ÊïôÁ®ã", "ËØæÁ®ã", "‰π¶", "Â≠¶‰π†", "ÊåáÂçó", "ÂèÇËÄÉ", "Ê∏ÖÂçï", "ËµÑÊñô", "ÂÖ•Èó®", "Ëã±ËØ≠"},
        "regex": [re.compile(r"\b(tutorial|course|book|papers|for beginners|curated list|collection of)\b", re.I)],
    },
}


_PRIORITY = {c.key: i for i, c in enumerate(CATEGORIES)}


def classify_repo(*, name: str, language: str | None, topics: list[str], description: str | None) -> dict:
    # Language is intentionally not used as a primary signal:
    # it tends to distort domain classification (e.g. many Go projects are not "build tools").
    text = f"{name or ''} {description or ''}".strip().lower()
    tokens = _tokenize(text)
    topics_l = {t.lower() for t in (topics or [])}

    score = {c.key: 0 for c in CATEGORIES}

    for cat_key, rule in RULES.items():
        score[cat_key] += 8 * len(topics_l.intersection(rule.get("topics", set())))
        score[cat_key] += 3 * len(tokens.intersection(rule.get("tokens", set())))
        for p in rule.get("phrases", set()):
            if _phrase_match(phrase=p, text=text, tokens=tokens):
                score[cat_key] += 3
        for rx in rule.get("regex", []):
            if rx.search(text):
                score[cat_key] += 4

    # Cross-category compound boosts (improves precision for common overlaps)
    ai_indicators = {"ai", "llm", "llms", "chatgpt", "gpt", "openai", "ollama", "agent", "agents", "agentic", "rag"}
    ai_app_indicators = {"assistant", "chat", "client", "webui", "desktop", "ui", "gui", "copilot"}
    # Prefer description/name signals over topics: some non-AI tools may tag ChatGPT etc. for marketing/integration.
    ai_app_topic_indicators = {"webui", "ui", "desktop", "tauri", "self-hosted", "client", "chat", "chatbot", "assistant"}
    ai_boost_indicators = (tokens & ai_indicators) or (topics_l & {"ai", "llm", "llms", "ollama", "openai", "stable-diffusion"})
    if ai_boost_indicators and ((tokens & ai_app_indicators) or (topics_l & ai_app_topic_indicators)):
        score["ai_apps"] += 14
        # If it looks like an end-user UI/client, down-weight inference category even when topics mention runtimes.
        score["ai_inference"] = int(score["ai_inference"] * 0.35)
        score["ai_rag_agent"] = int(score["ai_rag_agent"] * 0.75)

    # Learning-first hint: courses/books/tutorials should not be dominated by AI keywords.
    learning_indicators = {"tutorial", "course", "courses", "lesson", "lessons", "beginners", "beginner", "handbook", "reference", "book", "books", "papers"}
    if (tokens & learning_indicators) or ("ÊïôÁ®ã" in text) or ("ËØæÁ®ã" in text) or ("ÂÖ•Èó®" in text):
        score["learning"] += 12
        score["ai_inference"] = int(score["ai_inference"] * 0.6)
        score["ai_apps"] = int(score["ai_apps"] * 0.7)

    ai_platform_indicators = {"platform", "workflow", "orchestration", "studio", "visual", "builder", "low-code", "nocode", "agentic"}
    ai_platform_names = {"dify", "langflow", "flowise", "fastgpt"}
    if tokens & ai_platform_names:
        score["ai_platform"] += 20
    if (tokens & ai_indicators or topics_l & ai_indicators) and (tokens & ai_platform_indicators or ("Â∑•‰ΩúÊµÅ" in text) or ("ÁºñÊéí" in text) or ("‰Ωé‰ª£Á†Å" in text)):
        score["ai_platform"] += 12
    if tokens & ai_platform_indicators:
        score["ai_rag_agent"] = int(score["ai_rag_agent"] * 0.75)

    db_indicators = {"database", "dbms", "sql", "postgres", "postgresql", "mysql"}
    db_tool_indicators = {"orm", "migration", "client", "driver", "connector", "parser", "transpiler", "admin", "tool", "toolkit", "sqlx", "sea-orm", "sqlglot"}
    if (tokens & db_indicators or topics_l & db_indicators or ("Êï∞ÊçÆÂ∫ì" in text)) and (tokens & db_tool_indicators):
        score["db_tooling"] += 8
        score["db_systems"] = int(score["db_systems"] * 0.65)

    # Embedded / KV databases should prefer db_storage even if other DB signals exist.
    embedded_kv_topics = {"embedded-kv", "kv", "key-value", "embedded-database", "storage-engine", "lsm-tree", "rocksdb"}
    embedded_kv_tokens = {"key-value", "kv", "lsm", "rocksdb"}
    if ("embedded" in tokens or "in-process" in tokens) and ("database" in tokens or "databases" in tokens or "db" in tokens) and (
        (tokens & embedded_kv_tokens) or (topics_l & embedded_kv_topics) or ("ÂµåÂÖ•ÂºèÊï∞ÊçÆÂ∫ì" in text)
    ):
        score["db_storage"] += 12
        score["db_systems"] = int(score["db_systems"] * 0.6)
        score["db_tooling"] = int(score["db_tooling"] * 0.6)

    # Vector databases should prefer vector_search over generic db_systems.
    if (topics_l & {"vector-database", "vector-search"}) or (("vector" in tokens) and ("database" in tokens)):
        score["vector_search"] += 12
        score["db_systems"] = int(score["db_systems"] * 0.7)

    # Avoid classifying UI apps as frameworks
    if "ui_frameworks" in score and ("application" in tokens or "app" in tokens or "ÂÆ¢Êà∑Á´Ø" in text or "Ê°åÈù¢Â∫îÁî®" in text):
        score["ui_frameworks"] = int(score["ui_frameworks"] * 0.75)

    max_score = max(score.values()) if score else 0
    if max_score <= 0:
        # Fallback heuristics (best-effort, avoid defaulting to the first category)
        if {"tutorial", "course", "book", "papers", "reference", "algorithm", "algorithms"} & tokens:
            best_key = "learning"
        elif {"kernel", "os", "wsl", "vm", "hypervisor", "libos"} & tokens or ("Êìç‰ΩúÁ≥ªÁªü" in text) or ("ÂÜÖÊ†∏" in text):
            best_key = "systems_os"
        elif {"benchmark", "bench", "profil", "perf", "fio", "ior"} & tokens or ("Âü∫ÂáÜ" in text) or ("ÂéãÊµã" in text):
            best_key = "observability_perf"
        elif {"iptv", "music", "video"} & tokens or ("Èü≥‰πê" in text) or ("ËßÜÈ¢ë" in text):
            best_key = "media"
        elif language in {"Rust", "C", "C++", "Zig"} or "library" in tokens or "crate" in tokens:
            best_key = "systems_libs"
        else:
            best_key = "apps_productivity"
    else:
        best_key = max(score.keys(), key=lambda k: (score[k], -_PRIORITY.get(k, 10_000), k))
    sorted_keys = sorted(score.keys(), key=lambda k: score[k], reverse=True)
    tags = [k for k in sorted_keys if score[k] > 0][:3]
    return {"primary": best_key, "scores": score, "tags": tags}


def streak_from_days(days_sorted: list[dict]) -> dict:
    max_streak = 0
    max_start = None
    max_end = None
    current = 0
    current_start = None
    prev_date = None

    for d in days_sorted:
        date = dt.date.fromisoformat(d["date"])
        if d["count"] > 0:
            if current == 0:
                current_start = date
            current += 1
        else:
            if current > max_streak:
                max_streak = current
                max_start = current_start
                max_end = prev_date
            current = 0
            current_start = None
        prev_date = date

    if current > max_streak:
        max_streak = current
        max_start = current_start
        max_end = prev_date

    return {
        "count": max_streak,
        "start": max_start.isoformat() if max_start else None,
        "end": max_end.isoformat() if max_end else None,
    }


def main():
    user = load(RAW / "user.json")
    login = user["login"]

    repos = load(RAW / "user_repos.json")
    own_repos = [r for r in repos if not r.get("fork")]
    first_repo = min(own_repos, key=lambda r: r.get("created_at") or "9999") if own_repos else None

    contrib = load(RAW / "contributions.json")
    cc = contrib["data"]["user"]["contributionsCollection"]
    calendar = cc["contributionCalendar"]
    weeks = calendar["weeks"]

    days = []
    for week in weeks:
        for day in week.get("contributionDays", []):
            days.append(
                {
                    "date": day["date"],
                    "count": safe_int(day["contributionCount"]),
                    "weekday": safe_int(day["weekday"]),
                }
            )

    days_sorted = sorted(days, key=lambda d: d["date"])
    active_days = sum(1 for d in days_sorted if d["count"] > 0)
    total_days = len(days_sorted)
    inactive_days = total_days - active_days

    busiest_count = max((d["count"] for d in days_sorted), default=0)
    busiest_day = next((d for d in days_sorted if d["count"] == busiest_count), {"date": None, "count": 0})
    top_days = sorted([d for d in days_sorted if d["count"] > 0], key=lambda d: d["count"], reverse=True)[:8]

    streak = streak_from_days(days_sorted)

    weekday_sums = [0] * 7
    for d in days_sorted:
        weekday_sums[d["weekday"]] += d["count"]
    weekend_sum = weekday_sums[0] + weekday_sums[6]
    total_contrib = sum(weekday_sums)
    weekend_ratio = (weekend_sum / total_contrib) if total_contrib else 0.0
    if total_contrib == 0:
        pattern = "ÊöÇÊó†Êï∞ÊçÆ"
    elif weekend_ratio >= 0.40:
        pattern = "Âë®Êú´ÊàòÂ£´"
    elif weekend_ratio <= 0.20:
        pattern = "Â∑•‰ΩúÊó•ÈáçÂ∫¶"
    else:
        pattern = "ÂùáË°°Âûã"

    # Monthly activity
    month_contrib = defaultdict(int)
    month_active_days = defaultdict(int)
    for d in days_sorted:
        month = d["date"][:7]
        month_contrib[month] += d["count"]
        if d["count"] > 0:
            month_active_days[month] += 1
    month_contrib_sorted = sorted(month_contrib.items())
    most_active_month = max(month_contrib.items(), key=lambda kv: kv[1])[0] if month_contrib else None

    # Starred repos (all-time) + 2024 slice
    star_pages = load(RAW / "starred_repos_pages.json")
    star_edges = []
    for page in star_pages:
        star_edges.extend(page["data"]["user"]["starredRepositories"]["edges"])

    stars = []
    for edge in star_edges:
        node = edge["node"]
        stars.append(
            {
                "starredAt": edge["starredAt"],
                "nameWithOwner": node["nameWithOwner"],
                "description": node.get("description"),
                "stargazerCount": safe_int(node.get("stargazerCount")),
                "forkCount": safe_int(node.get("forkCount")),
                "primaryLanguage": (node.get("primaryLanguage") or {}).get("name"),
                "topics": [t["topic"]["name"] for t in (node.get("repositoryTopics") or {}).get("nodes", [])],
                "url": f"https://github.com/{node['nameWithOwner']}",
            }
        )

    stars_2024 = [s for s in stars if s["starredAt"].startswith(f"{YEAR}-")]
    stars_before = [s for s in stars if s["starredAt"] < f"{YEAR}-01-01T00:00:00Z"]

    stars_by_year = defaultdict(int)
    for s in stars:
        year = s["starredAt"][:4]
        stars_by_year[year] += 1

    star_month_counts = Counter([s["starredAt"][:7] for s in stars_2024])
    star_month_top_repo = {}
    for s in stars_2024:
        key = s["starredAt"][:7]
        prev = star_month_top_repo.get(key)
        if not prev or s["stargazerCount"] > prev["stargazerCount"]:
            star_month_top_repo[key] = s

    star_month_repos = defaultdict(list)
    for s in stars_2024:
        star_month_repos[s["starredAt"][:7]].append(s)
    for month_key in star_month_repos:
        star_month_repos[month_key].sort(key=lambda x: (x.get("stargazerCount", 0), x.get("starredAt") or ""), reverse=True)

    # Star events by month (for timeline charts)
    star_month_events = defaultdict(list)
    for s in stars_2024:
        key = s["starredAt"][:7]
        star_month_events[key].append(
            {
                "starredAt": s["starredAt"],
                "nameWithOwner": s["nameWithOwner"],
                "stars": safe_int(s.get("stargazerCount")),
                "url": s.get("url"),
            }
        )
    for month_key in star_month_events:
        star_month_events[month_key].sort(key=lambda x: x.get("starredAt") or "")

    star_hour_local = [0] * 24
    for s in stars_2024:
        hour = to_local(s["starredAt"]).hour
        star_hour_local[hour] += 1

    star_lang_2024 = Counter([s["primaryLanguage"] for s in stars_2024 if s.get("primaryLanguage")])
    star_topic_2024 = Counter()
    for s in stars_2024:
        for t in s.get("topics", []):
            star_topic_2024[t.lower()] += 1

    star_topic_before = Counter()
    for s in stars_before:
        for t in s.get("topics", []):
            star_topic_before[t.lower()] += 1

    new_topics = []
    rising_topics = []
    for topic, cur in star_topic_2024.most_common():
        prev = star_topic_before.get(topic, 0)
        if prev == 0 and cur >= 3:
            new_topics.append({"topic": topic, "count2024": cur, "countBefore": 0})
        elif prev > 0 and cur >= max(5, prev * 2):
            rising_topics.append(
                {
                    "topic": topic,
                    "count2024": cur,
                    "countBefore": prev,
                    "ratio": round(cur / prev, 2) if prev else None,
                }
            )

    new_topics = new_topics[:12]
    rising_topics = sorted(rising_topics, key=lambda x: (-(x["ratio"] or 0), -x["count2024"]))[:12]

    lang_before = Counter([s["primaryLanguage"] for s in stars_before if s.get("primaryLanguage")])
    new_langs = []
    for lang, cur in star_lang_2024.most_common():
        prev = lang_before.get(lang, 0)
        if prev == 0 and cur >= 2:
            new_langs.append({"language": lang, "count2024": cur, "countBefore": 0})
    new_langs = new_langs[:8]

    first_star_ever = min(stars, key=lambda s: s["starredAt"]) if stars else None
    first_star_2024 = min(stars_2024, key=lambda s: s["starredAt"]) if stars_2024 else None
    latest_star_2024 = max(stars_2024, key=lambda s: s["starredAt"]) if stars_2024 else None

    # Repo creation by year (own repos)
    repos_created_by_year = Counter([(r.get("created_at") or "")[:4] for r in own_repos if r.get("created_at")])

    # Contribution repos (top)
    commit_repos = cc.get("commitContributionsByRepository", [])
    pr_repos = cc.get("pullRequestContributionsByRepository", [])
    issue_repos = cc.get("issueContributionsByRepository", [])

    commit_repo_list = [
        {
            "nameWithOwner": item["repository"]["nameWithOwner"],
            "count": safe_int(item["contributions"]["totalCount"]),
        }
        for item in commit_repos
    ]
    pr_repo_list = [
        {
            "nameWithOwner": item["repository"]["nameWithOwner"],
            "count": safe_int(item["contributions"]["totalCount"]),
        }
        for item in pr_repos
    ]
    issue_repo_list = [
        {
            "nameWithOwner": item["repository"]["nameWithOwner"],
            "count": safe_int(item["contributions"]["totalCount"]),
        }
        for item in issue_repos
    ]

    # Focus project (most commit contributions in own repos)
    focus_project = None
    for item in sorted(commit_repo_list, key=lambda x: x["count"], reverse=True):
        if item["nameWithOwner"].lower().startswith(login.lower() + "/"):
            focus_project = item
            break

    # Merged PRs (2024)
    pr_pages = load(RAW / "prs_2024_pages.json")
    merged_prs = []
    for page in pr_pages:
        for node in page["data"]["search"]["nodes"]:
            if not node:
                continue
            merged_prs.append(
                {
                    "title": node["title"],
                    "url": node["url"],
                    "createdAt": node.get("createdAt"),
                    "mergedAt": node.get("mergedAt"),
                    "additions": safe_int(node.get("additions")),
                    "deletions": safe_int(node.get("deletions")),
                    "repo": node["repository"]["nameWithOwner"],
                    "repoStars": safe_int(node["repository"].get("stargazerCount")),
                    "repoLanguage": (node["repository"].get("primaryLanguage") or {}).get("name"),
                    "repoTopics": [
                        t["topic"]["name"]
                        for t in (node["repository"].get("repositoryTopics") or {}).get("nodes", [])
                    ],
                }
            )

    pr_total = len(merged_prs)
    pr_add_total = sum(pr["additions"] for pr in merged_prs)
    pr_del_total = sum(pr["deletions"] for pr in merged_prs)
    pr_lines_total = pr_add_total + pr_del_total

    latest_pr_created = None
    if merged_prs:
        latest_pr_created = max(
            merged_prs,
            key=lambda pr: pr["createdAt"] or "0000",
        )

    biggest_pr = None
    if merged_prs:
        biggest_pr = max(merged_prs, key=lambda pr: pr["additions"] + pr["deletions"])

    pr_by_repo = defaultdict(lambda: {"count": 0, "lines": 0, "add": 0, "del": 0})
    for pr in merged_prs:
        agg = pr_by_repo[pr["repo"]]
        agg["count"] += 1
        agg["add"] += pr["additions"]
        agg["del"] += pr["deletions"]
        agg["lines"] += pr["additions"] + pr["deletions"]

    oss_award_repo = None
    if pr_by_repo:
        oss_award_repo = max(pr_by_repo.items(), key=lambda kv: (kv[1]["count"], kv[1]["lines"]))[0]

    oss_award = None
    if oss_award_repo:
        oss_award = {"repo": oss_award_repo, **pr_by_repo[oss_award_repo]}

    # External contributed repos sample
    contrib_pages = load(RAW / "contributed_repos_pages.json")
    contributed_repos = []
    for page in contrib_pages:
        contributed_repos.extend(page["data"]["user"]["repositoriesContributedTo"]["nodes"])
    external_contrib = [
        r
        for r in contributed_repos
        if (r.get("owner") or {}).get("login", "").lower() != login.lower()
    ]
    external_contrib_sorted = sorted(external_contrib, key=lambda r: safe_int(r.get("stargazerCount")), reverse=True)

    # Holiday stars (Chinese-focused, 2024)
    spring_festival = dt.date(2024, 2, 10)
    chuxi = spring_festival - dt.timedelta(days=1)
    holidays = [
        ("new_year", "ÂÖÉÊó¶", dt.date(2024, 1, 1)),
        ("chuxi", "Èô§Â§ï", chuxi),
        ("spring_festival", "Êò•ËäÇ", spring_festival),
        ("qingming", "Ê∏ÖÊòéËäÇ", dt.date(2024, 4, 4)),
        ("labor_day", "Âä≥Âä®ËäÇ", dt.date(2024, 5, 1)),
        ("dragon_boat", "Á´ØÂçàËäÇ", dt.date(2024, 6, 10)),
        ("qixi", "‰∏ÉÂ§ï", dt.date(2024, 8, 10)),
        ("national_day", "ÂõΩÂ∫ÜËäÇ", dt.date(2024, 10, 1)),
        ("mid_autumn", "‰∏≠ÁßãËäÇ", dt.date(2024, 9, 17)),
        ("programmer_day", "Á®ãÂ∫èÂëòËäÇ", dt.date(2024, 10, 24)),
        ("singles_day", "ÂèåÂçÅ‰∏Ä", dt.date(2024, 11, 11)),
        ("new_year_eve", "Ë∑®Âπ¥Â§ú(12/31)", dt.date(2024, 12, 31)),
    ]

    holiday_cards = []
    for key, label, date in holidays:
        repos = [s for s in stars_2024 if s["starredAt"][:10] == date.isoformat()]
        if not repos:
            continue
        repos_sorted = sorted(repos, key=lambda s: s["stargazerCount"], reverse=True)
        holiday_cards.append(
            {
                "key": key,
                "label": label,
                "date": date.isoformat(),
                "count": len(repos_sorted),
                "repos": [
                    {
                        "nameWithOwner": r["nameWithOwner"],
                        "stars": r["stargazerCount"],
                        "language": r.get("primaryLanguage"),
                        "url": r.get("url"),
                    }
                    for r in repos_sorted[:6]
                ],
            }
        )

    # Category stats based on 2024 stars
    stars_2024_with_cat = []
    category_counts = Counter()
    for s in stars_2024:
        cat = classify_repo(
            name=s["nameWithOwner"],
            language=s.get("primaryLanguage"),
            topics=s.get("topics", []),
            description=s.get("description"),
        )
        stars_2024_with_cat.append({**s, "category": cat})
        category_counts[cat["primary"]] += 1

    category_top_repos = defaultdict(list)
    for s in stars_2024_with_cat:
        category_top_repos[s["category"]["primary"]].append(s)
    for key in category_top_repos:
        category_top_repos[key].sort(key=lambda s: s.get("stargazerCount", 0), reverse=True)

    # Contribution-side category aggregation (merged PRs)
    pr_category_lines = Counter()
    pr_category_count = Counter()
    for pr in merged_prs:
        cat = classify_repo(
            name=pr["repo"],
            language=pr.get("repoLanguage"),
            topics=pr.get("repoTopics", []),
            description=None,
        )
        pr_category_count[cat["primary"]] += 1
        pr_category_lines[cat["primary"]] += pr["additions"] + pr["deletions"]

    # Radar values: combine star count + PR lines signal
    max_star_cat = max(category_counts.values(), default=0)
    max_pr_lines_cat = max(pr_category_lines.values(), default=0)

    radar = {}
    for cat in CATEGORIES:
        star_score = normalize(category_counts.get(cat.key, 0), max_star_cat)
        pr_score = normalize(pr_category_lines.get(cat.key, 0), max_pr_lines_cat)
        value = (0.65 * star_score) + (0.35 * pr_score)
        radar[cat.key] = int(round(clamp(value, 0.0, 1.0) * 100))

    primary_track = max(radar.items(), key=lambda kv: kv[1])[0] if radar else "systems"

    # 90-day events: deep-night push (best-effort)
    events = load(RAW / "events_90d.json") if (RAW / "events_90d.json").exists() else []
    push_events = [e for e in events if e.get("type") == "PushEvent"]

    def night_score(local_dt: dt.datetime) -> float:
        # Higher is "deeper night": prefer 00:00-05:59 and 23:00-23:59
        h = local_dt.hour + local_dt.minute / 60.0
        if h >= 23:
            return 1.0 + (h - 23) / 1.0
        if h < 6:
            return 1.0 + (6 - h) / 6.0
        return 0.0

    deep_night_push = None
    if push_events:
        def key_fn(ev):
            local_dt = to_local(ev.get("created_at"))
            return (night_score(local_dt), local_dt.isoformat())

        deep_night_push = max(push_events, key=key_fn)

    deep_night_push_card = None
    if deep_night_push:
        local_dt = to_local(deep_night_push.get("created_at"))
        commits = (deep_night_push.get("payload") or {}).get("commits") or []
        deep_night_push_card = {
            "localTime": local_dt.isoformat(),
            "repo": (deep_night_push.get("repo") or {}).get("name"),
            "commitCount": len(commits),
            "sampleMessages": [c.get("message", "")[:80] for c in commits[:3]],
        }

    # Identity (avoid over-assertive single label)
    primary_langs = [lang for lang, _ in Counter([r.get("language") for r in own_repos if r.get("language")]).most_common(5)]
    top_topics = [t for t, _ in star_topic_2024.most_common(8)]

    identity_lines = []
    if primary_langs:
        identity_lines.append(" / ".join(primary_langs[:3]))
    if "rust" in top_topics:
        identity_lines.append("Rust ÁîüÊÄÅÈáçÂ∫¶ÂÖ≥Ê≥®")
    if "ai" in top_topics or "llm" in top_topics:
        identity_lines.append("AI Â∑•ÂÖ∑ÈìæÊé¢Á¥¢")
    if any(repo["nameWithOwner"].startswith("apache/datafusion") for repo in external_contrib_sorted[:10]) or (oss_award_repo == "apache/datafusion"):
        identity_lines.append("DataFusion ÂºÄÊ∫êË¥°ÁåÆ")

    identity = " ¬∑ ".join(identity_lines[:3]) if identity_lines else "ÂºÄÂèëËÄÖ"

    # Meet GitHub duration (as of 2024-12-31)
    created_date = parse_iso8601(user.get("created_at")).date()
    report_end = dt.date(YEAR, 12, 31)
    days_since = (report_end - created_date).days
    years_approx = round(days_since / 365.2425, 1)

    dataset = {
        "meta": {
            "year": YEAR,
            "timezone": "Asia/Shanghai",
            "generatedAt": dt.datetime.now(tz=dt.timezone.utc).isoformat(),
            "dataProvenance": {
                "rawDir": str(RAW),
                "notes": [
                    "ÊâÄÊúâÊï∞ÂÄºÊù•Ëá™ gh api ÁöÑÂéüÂßã JSONÔºà‰ªìÂ∫ìÂÜÖÂ∑≤‰øùÂ≠òÔºâÔºåÈ°µÈù¢‰ªÖÂÅöÁªüËÆ°‰∏éÂèØËßÜÂåñ„ÄÇ",
                    "GitHub Events API ‰ªÖ‰øùÁïôËøë 90 Â§©ÔºõÊ∑±Â§úÊèê‰∫§ÂΩ©Ëõã‰∏∫ best-effort„ÄÇ",
                    "‰ªìÂ∫ìÊî∂Âà∞ÁöÑ Stars/Forks ‰∏∫ÂΩìÂâçÂø´ÁÖßÔºå‰∏ç‰ª£Ë°® 2024 Êñ∞Â¢û„ÄÇ",
                ],
            },
        },
        "user": {
            "login": login,
            "name": user.get("name"),
            "avatarUrl": user.get("avatar_url"),
            "createdAt": user.get("created_at"),
            "followers": safe_int(user.get("followers")),
            "following": safe_int(user.get("following")),
            "meetGitHub": {
                "asOf": report_end.isoformat(),
                "days": days_since,
                "yearsApprox": years_approx,
            },
            "firstRepo": {
                "name": first_repo.get("name") if first_repo else None,
                "createdAt": first_repo.get("created_at") if first_repo else None,
            },
            "firstStarEver": {
                "starredAt": first_star_ever["starredAt"] if first_star_ever else None,
                "repo": first_star_ever["nameWithOwner"] if first_star_ever else None,
                "url": first_star_ever.get("url") if first_star_ever else None,
            },
            "identity": identity,
        },
        "year": {
            "totals": {
                "contributions": safe_int(calendar.get("totalContributions")),
                "commits": safe_int(cc.get("totalCommitContributions")),
                "prs": safe_int(cc.get("totalPullRequestContributions")),
                "issues": safe_int(cc.get("totalIssueContributions")),
                "reposContributedTo": safe_int(cc.get("totalRepositoryContributions")),
            },
            "activity": {
                "totalDays": total_days,
                "activeDays": active_days,
                "inactiveDays": inactive_days,
                "activeRate": round(active_days / total_days, 3) if total_days else 0,
                "busiestDay": busiest_day,
                "topDays": top_days,
                "longestStreak": streak,
                "weekdaySums": weekday_sums,
                "weekendRatio": round(weekend_ratio, 3),
                "patternLabel": pattern,
                "mostActiveMonth": most_active_month,
                "byMonth": [
                    {
                        "month": m,
                        "contributions": month_contrib[m],
                        "activeDays": month_active_days.get(m, 0),
                    }
                    for m, _ in month_contrib_sorted
                ],
            },
            "repos": {
                "total": len(repos),
                "ownTotal": len(own_repos),
                "createdByYear": dict(repos_created_by_year),
                "createdInYear": sum(1 for r in own_repos if (r.get("created_at") or "").startswith(f"{YEAR}-")),
                "ownStarsTotalSnapshot": sum(safe_int(r.get("stargazers_count")) for r in own_repos),
                "ownForksTotalSnapshot": sum(safe_int(r.get("forks_count")) for r in own_repos),
                "topSnapshot": [
                    {
                        "name": r.get("name"),
                        "stars": safe_int(r.get("stargazers_count")),
                        "forks": safe_int(r.get("forks_count")),
                        "language": r.get("language"),
                        "createdAt": r.get("created_at"),
                        "url": r.get("html_url"),
                    }
                    for r in sorted(own_repos, key=lambda r: safe_int(r.get("stargazers_count")), reverse=True)[:10]
                ],
            },
            "contributions": {
                "calendarWeeks": weeks,
                "topCommitRepos": sorted(commit_repo_list, key=lambda x: x["count"], reverse=True)[:12],
                "topPrRepos": sorted(pr_repo_list, key=lambda x: x["count"], reverse=True)[:12],
                "topIssueRepos": sorted(issue_repo_list, key=lambda x: x["count"], reverse=True)[:12],
                "focusProject": focus_project,
            },
            "stars": {
                "total2024": len(stars_2024),
                "totalAllTime": len(stars),
                "byYear": dict(sorted(stars_by_year.items())),
                "byMonth2024": [
                    {
                        "month": month,
                        "count": star_month_counts.get(month, 0),
                        "topRepo": {
                            "nameWithOwner": (star_month_top_repo.get(month) or {}).get("nameWithOwner"),
                            "stars": (star_month_top_repo.get(month) or {}).get("stargazerCount"),
                            "url": (star_month_top_repo.get(month) or {}).get("url"),
                        },
                        "repos": [
                            {
                                "nameWithOwner": r.get("nameWithOwner"),
                                "stars": safe_int(r.get("stargazerCount")),
                                "url": r.get("url"),
                            }
                            for r in (star_month_repos.get(month) or [])[:12]
                        ],
                        "events": [
                            {
                                "starredAt": e.get("starredAt"),
                                "nameWithOwner": e.get("nameWithOwner"),
                                "stars": safe_int(e.get("stars")),
                                "url": e.get("url"),
                            }
                            for e in (star_month_events.get(month) or [])
                        ],
                    }
                    for month, _ in sorted(month_contrib_sorted)
                ],
                "byHourLocal2024": star_hour_local,
                "topLanguages2024": [{"name": k, "count": v} for k, v in star_lang_2024.most_common(12)],
                "topTopics2024": [{"name": k, "count": v} for k, v in star_topic_2024.most_common(24)],
                "topStarredRepos2024": [
                    {
                        "nameWithOwner": s["nameWithOwner"],
                        "stars": s["stargazerCount"],
                        "language": s.get("primaryLanguage"),
                        "starredAt": s["starredAt"],
                        "url": s["url"],
                    }
                    for s in sorted(stars_2024, key=lambda s: s["stargazerCount"], reverse=True)[:20]
                ],
                "firstStar2024": {
                    "starredAt": first_star_2024["starredAt"] if first_star_2024 else None,
                    "repo": first_star_2024["nameWithOwner"] if first_star_2024 else None,
                    "url": first_star_2024.get("url") if first_star_2024 else None,
                },
                "latestStar2024": {
                    "starredAt": latest_star_2024["starredAt"] if latest_star_2024 else None,
                    "repo": latest_star_2024["nameWithOwner"] if latest_star_2024 else None,
                    "url": latest_star_2024.get("url") if latest_star_2024 else None,
                },
            },
            "discoveries": {
                "newTopics": new_topics,
                "risingTopics": rising_topics,
                "newLanguages": new_langs,
            },
            "categories": {
                "definitions": [
                    {
                        "key": c.key,
                        "label": c.label,
                        "emoji": c.emoji,
                        "color": c.color,
                        "description": c.description,
                    }
                    for c in CATEGORIES
                ],
                "starCounts2024": {k: category_counts.get(k, 0) for k in CATEGORY_BY_KEY.keys()},
                "topRepos2024": {
                    k: [
                        {
                            "nameWithOwner": s["nameWithOwner"],
                            "stars": s["stargazerCount"],
                            "language": s.get("primaryLanguage"),
                            "url": s["url"],
                        }
                        for s in category_top_repos.get(k, [])
                    ]
                    for k in CATEGORY_BY_KEY.keys()
                },
                "radar": radar,
                "primaryTrack": primary_track,
                "prCountsByCategory": dict(pr_category_count),
                "prLinesByCategory": dict(pr_category_lines),
            },
            "openSource": {
                "mergedPrs": {
                    "total": pr_total,
                    "additions": pr_add_total,
                    "deletions": pr_del_total,
                    "lines": pr_lines_total,
                },
                "ossAward": oss_award,
                "biggestPr": biggest_pr,
                "latestPrCreated": latest_pr_created,
                "highlights": sorted(merged_prs, key=lambda pr: pr["additions"] + pr["deletions"], reverse=True)[:8],
                "externalContributedRepos": [
                    {
                        "nameWithOwner": r.get("nameWithOwner"),
                        "stars": safe_int(r.get("stargazerCount")),
                        "language": (r.get("primaryLanguage") or {}).get("name"),
                        "topics": [t["topic"]["name"] for t in (r.get("repositoryTopics") or {}).get("nodes", [])],
                        "owner": (r.get("owner") or {}).get("login"),
                    }
                    for r in external_contrib_sorted[:18]
                ],
            },
            "specialDates": {
                "holidayStars": holiday_cards,
                "deepNightPush90d": deep_night_push_card,
            },
        },
    }

    out_path = OUT_DIR / "dataset.json"
    out_path.write_text(json.dumps(dataset, ensure_ascii=False, indent=2), encoding="utf-8")
    print("dataset written to", out_path)


if __name__ == "__main__":
    main()
